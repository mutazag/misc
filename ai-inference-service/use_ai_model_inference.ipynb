{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run this model in Python\n",
    "\n",
    "> pip install openai\n",
    "\"\"\"\n",
    "import os\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider, AzureCliCredential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"DeepSeek-R1\": \"DeepSeek-R1\", \n",
    "    \"Phi-4-mini-instruct\": \"Phi-4-mini-instruct-hub\", \n",
    "    \"gpt-4o\": \"gpt-4o\",\n",
    "    \"o3-mini\": \"o3-mini\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.getenv(\"AZURE_INFERENCE_SDK_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint, \n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_INFERENCE_SDK_ENDPOINT_KEY\")),\n",
    "    credential_scopes=[\"https://cognitiveservices.azure.com/.default\"],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_think_tags(content):\n",
    "    \"\"\"\n",
    "    Parse the <think> </think> tags out of the response in content\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Find all <think>...</think> tags and their content\n",
    "    think_tags = re.findall(r\"<think>(.*?)</think>\", content)\n",
    "    # match = re.search(r\"<think>(.*?)</think>\", content)\n",
    "    # think_tags = match.group(0) if match else \"\"\n",
    "    \n",
    "    # Remove the <think>...</think> tags from the content\n",
    "    content_without_think_tags = re.sub(r\"<think>.*?</think>\", \"\", content)\n",
    "    \n",
    "    return think_tags, content_without_think_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    UserMessage(content=\"One shirt takes 3 hours to make. How long will it take to make 10 shirts?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek-R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(Timeout) The operation was timeout.\nCode: Timeout\nMessage: The operation was timeout.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHttpResponseError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDeepSeek-R1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\git\\misc\\ai-inference-service\\.venv\\Lib\\site-packages\\azure\\ai\\inference\\_patch.py:738\u001b[39m, in \u001b[36mChatCompletionsClient.complete\u001b[39m\u001b[34m(self, body, messages, stream, frequency_penalty, presence_penalty, temperature, top_p, max_tokens, response_format, stop, tools, tool_choice, seed, model, model_extras, **kwargs)\u001b[39m\n\u001b[32m    736\u001b[39m         response.read()  \u001b[38;5;66;03m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[32m    737\u001b[39m     map_error(status_code=response.status_code, response=response, error_map=error_map)\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response)\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _models.StreamingChatCompletions(response)\n",
      "\u001b[31mHttpResponseError\u001b[39m: (Timeout) The operation was timeout.\nCode: Timeout\nMessage: The operation was timeout."
     ]
    }
   ],
   "source": [
    "response = client.complete( \n",
    "    messages = messages, \n",
    "    model = models['DeepSeek-R1']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The problem says that one shirt takes 3 hours to make, and we need to find out how long it will take to make 10 shirts. Hmm, seems straightforward, but let me think through it step by step.\n",
      "\n",
      "First, if making one shirt requires 3 hours, then for 10 shirts, I probably need to multiply the time per shirt by the number of shirts. That would be 3 hours per shirt times 10 shirts. So, 3 times 10... that equals 30 hours. Wait, but is there anything else I need to consider here?\n",
      "\n",
      "Let me visualize the scenario. If I have one person working on the shirts, each shirt takes 3 hours, right? So, making them one after another would indeed take 30 hours total. But maybe the question is implying if there's a way to reduce the time by working on multiple shirts at the same time? However, usually, when such problems are presented, especially in basic math, they assume that the work is done sequentially unless stated otherwise. The problem doesn't mention anything about multiple workers or parallel production, so I think it's safe to stick with the straightforward multiplication.\n",
      "\n",
      "But just to double-check, let's break it down. Let's say the person starts making the first shirt at time 0. After 3 hours, they finish the first shirt. Then they start the second shirt, which would take another 3 hours, finishing at 6 hours. Continuing this pattern, the third shirt would be done at 9 hours, the fourth at 12, and so on. So the 10th shirt would be completed at 3*10 = 30 hours. Yep, that lines up.\n",
      "\n",
      "Alternatively, maybe there's a trick to the question that I'm missing? Like setup time or something else? But the problem statement is very brief. It just mentions the time to make one shirt. It doesn't mention any additional time factors or efficiencies gained when making multiple shirts. So I don't think we need to consider those complexities here.\n",
      "\n",
      "Another way to look at it is through rates. The rate at which shirts are made is 1 shirt per 3 hours, so the rate is 1/3 shirts per hour. To find the time (T) needed to make 10 shirts, we can use the formula:\n",
      "\n",
      "Number of shirts = rate * time\n",
      "\n",
      "So 10 = (1/3) * T\n",
      "\n",
      "Solving for T gives T = 10 / (1/3) = 10 * 3 = 30 hours. That confirms the same answer.\n",
      "\n",
      "Wait, but what if multiple people are working on the shirts? The question doesn't specify the number of workers. Hmm, that could be a different interpretation. If the problem is asking how long it takes one person to make 10 shirts, the answer is 30 hours. If they have multiple people working simultaneously, the time could be reduced. But since the problem doesn't mention multiple workers, it's standard to assume it's one person working alone. If they wanted to consider multiple workers, they would have specified, right?\n",
      "\n",
      "For example, if two people are working together, each making shirts, the time would be halved. But again, the problem doesn't mention that. So, sticking with the original thought process, the answer is 30 hours.\n",
      "\n",
      "I think that's solid. No other factors seem to be in play here. The key is that each shirt takes 3 hours of work, and without any concurrency or additional workers, the total time adds up linearly. So, 10 shirts would take 10 times 3 hours, which is 30 hours. Yeah, that makes sense.\n",
      "\n",
      "Just to recap: 1 shirt = 3 hours. For each additional shirt, add 3 hours. So 10 shirts would be 3 + 3 + ... + 3 (ten times) = 3*10 = 30. Perfect. No hidden steps or tricks here. The answer should be 30 hours.\n",
      "</think>\n",
      "\n",
      "To determine the total time required to make 10 shirts, multiply the time per shirt by the number of shirts:\n",
      "\n",
      "\\[\n",
      "3\\ \\text{hours/shirt} \\times 10\\ \\text{shirts} = 30\\ \\text{hours}\n",
      "\\]\n",
      "\n",
      "**Answer:** It will take 30 hours to make 10 shirts.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If it takes **3 hours** to make one shirt, then to make **10 shirts**, you simply multiply the time by the number of shirts:\n",
      "\n",
      "\\[\n",
      "3 \\, \\text{hours} \\times 10 \\, \\text{shirts} = 30 \\, \\text{hours}.\n",
      "\\]\n",
      "\n",
      "So, it will take **30 hours** to make 10 shirts.\n"
     ]
    }
   ],
   "source": [
    "# gpt-4o\n",
    "response = client.complete(\n",
    "    messages = messages, \n",
    "    model = models[\"gpt-4o\"]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-4-mini-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(unknown_model) Unknown model: phi-4-mini-instruct-hub\nCode: unknown_model\nMessage: Unknown model: phi-4-mini-instruct-hub",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHttpResponseError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPhi-4-mini-instruct\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\git\\misc\\ai-inference-service\\.venv\\Lib\\site-packages\\azure\\ai\\inference\\_patch.py:738\u001b[39m, in \u001b[36mChatCompletionsClient.complete\u001b[39m\u001b[34m(self, body, messages, stream, frequency_penalty, presence_penalty, temperature, top_p, max_tokens, response_format, stop, tools, tool_choice, seed, model, model_extras, **kwargs)\u001b[39m\n\u001b[32m    736\u001b[39m         response.read()  \u001b[38;5;66;03m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[32m    737\u001b[39m     map_error(status_code=response.status_code, response=response, error_map=error_map)\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response)\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _models.StreamingChatCompletions(response)\n",
      "\u001b[31mHttpResponseError\u001b[39m: (unknown_model) Unknown model: phi-4-mini-instruct-hub\nCode: unknown_model\nMessage: Unknown model: phi-4-mini-instruct-hub"
     ]
    }
   ],
   "source": [
    "response = client.complete(\n",
    "    messages = messages, \n",
    "    model = models['Phi-4-mini-instruct']\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# o3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceNotFoundError",
     "evalue": "(404) Resource not found\nCode: 404\nMessage: Resource not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceNotFoundError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m client = ChatCompletionsClient(\n\u001b[32m      2\u001b[39m     endpoint=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mAZURE_OPENAI_ENDPOINT\u001b[39m\u001b[33m\"\u001b[39m), \n\u001b[32m      3\u001b[39m     credential=AzureKeyCredential(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mAZURE_OPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mo3-mini\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\git\\misc\\ai-inference-service\\.venv\\Lib\\site-packages\\azure\\ai\\inference\\_patch.py:737\u001b[39m, in \u001b[36mChatCompletionsClient.complete\u001b[39m\u001b[34m(self, body, messages, stream, frequency_penalty, presence_penalty, temperature, top_p, max_tokens, response_format, stop, tools, tool_choice, seed, model, model_extras, **kwargs)\u001b[39m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[32m    736\u001b[39m         response.read()  \u001b[38;5;66;03m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m     \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response)\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\git\\misc\\ai-inference-service\\.venv\\Lib\\site-packages\\azure\\core\\exceptions.py:163\u001b[39m, in \u001b[36mmap_error\u001b[39m\u001b[34m(status_code, response, error_map)\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    162\u001b[39m error = error_type(response=response)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[31mResourceNotFoundError\u001b[39m: (404) Resource not found\nCode: 404\nMessage: Resource not found"
     ]
    }
   ],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_OPENAI_API_KEY\"))\n",
    ")\n",
    "response = client.complete(\n",
    "    messages = messages, \n",
    "    model = models['o3-mini']\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
