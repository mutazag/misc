{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dd78e3",
   "metadata": {},
   "source": [
    "# Analyze Image with Content Filter\n",
    "\n",
    "https://learn.microsoft.com/en-us/rest/api/contentsafety/image-operations/analyze-image?view=rest-contentsafety-2024-09-01&tabs=HTTP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c29bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import enum\n",
    "import json\n",
    "import requests\n",
    "from typing import Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff7f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MediaType(enum.Enum):\n",
    "    Text = 1\n",
    "    Image = 2\n",
    "\n",
    "\n",
    "class Category(enum.Enum):\n",
    "    Hate = 1\n",
    "    SelfHarm = 2\n",
    "    Sexual = 3\n",
    "    Violence = 4\n",
    "\n",
    "\n",
    "class Action(enum.Enum):\n",
    "    Accept = 1\n",
    "    Reject = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819393d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionError(Exception):\n",
    "    def __init__(self, code: str, message: str) -> None:\n",
    "        \"\"\"\n",
    "        Exception raised when there is an error in detecting the content.\n",
    "\n",
    "        Args:\n",
    "        - code (str): The error code.\n",
    "        - message (str): The error message.\n",
    "        \"\"\"\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"DetectionError(code={self.code}, message={self.message})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c028fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decision(object):\n",
    "    def __init__(\n",
    "        self, suggested_action: Action, action_by_category: dict[Category, Action]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Represents the decision made by the content moderation system.\n",
    "\n",
    "        Args:\n",
    "        - suggested_action (Action): The suggested action to take.\n",
    "        - action_by_category (dict[Category, Action]): The action to take for each category.\n",
    "        \"\"\"\n",
    "        self.suggested_action = suggested_action\n",
    "        self.action_by_category = action_by_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6665a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContentSafety(object):\n",
    "    def __init__(self, endpoint: str, subscription_key: str, api_version: str) -> None:\n",
    "        \"\"\"\n",
    "        Creates a new ContentSafety instance.\n",
    "\n",
    "        Args:\n",
    "        - endpoint (str): The endpoint URL for the Content Safety API.\n",
    "        - subscription_key (str): The subscription key for the Content Safety API.\n",
    "        - api_version (str): The version of the Content Safety API to use.\n",
    "        \"\"\"\n",
    "        self.endpoint = endpoint\n",
    "        self.subscription_key = subscription_key\n",
    "        self.api_version = api_version\n",
    "\n",
    "    def build_url(self, media_type: MediaType) -> str:\n",
    "        \"\"\"\n",
    "        Builds the URL for the Content Safety API based on the media type.\n",
    "\n",
    "        Args:\n",
    "        - media_type (MediaType): The type of media to analyze.\n",
    "\n",
    "        Returns:\n",
    "        - str: The URL for the Content Safety API.\n",
    "        \"\"\"\n",
    "        if media_type == MediaType.Text:\n",
    "            return f\"{self.endpoint}/contentsafety/text:analyze?api-version={self.api_version}\"\n",
    "        elif media_type == MediaType.Image:\n",
    "            return f\"{self.endpoint}/contentsafety/image:analyze?api-version={self.api_version}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid Media Type {media_type}\")\n",
    "\n",
    "    def build_headers(self) -> dict[str, str]:\n",
    "        \"\"\"\n",
    "        Builds the headers for the Content Safety API request.\n",
    "\n",
    "        Returns:\n",
    "        - dict[str, str]: The headers for the Content Safety API request.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Ocp-Apim-Subscription-Key\": self.subscription_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def build_request_body(\n",
    "        self,\n",
    "        media_type: MediaType,\n",
    "        content: str,\n",
    "        blocklists: list[str],\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Builds the request body for the Content Safety API request.\n",
    "\n",
    "        Args:\n",
    "        - media_type (MediaType): The type of media to analyze.\n",
    "        - content (str): The content to analyze.\n",
    "        - blocklists (list[str]): The blocklists to use for text analysis.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The request body for the Content Safety API request.\n",
    "        \"\"\"\n",
    "        if media_type == MediaType.Text:\n",
    "            return {\n",
    "                \"text\": content,\n",
    "                \"blocklistNames\": blocklists,\n",
    "            }\n",
    "        elif media_type == MediaType.Image:\n",
    "            return {\"image\": {\"content\": content}}\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid Media Type {media_type}\")\n",
    "\n",
    "    def detect(\n",
    "        self,\n",
    "        media_type: MediaType,\n",
    "        content: str,\n",
    "        blocklists: list[str] = [],\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Detects unsafe content using the Content Safety API.\n",
    "\n",
    "        Args:\n",
    "        - media_type (MediaType): The type of media to analyze.\n",
    "        - content (str): The content to analyze.\n",
    "        - blocklists (list[str]): The blocklists to use for text analysis.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The response from the Content Safety API.\n",
    "        \"\"\"\n",
    "        url = self.build_url(media_type)\n",
    "        headers = self.build_headers()\n",
    "        request_body = self.build_request_body(media_type, content, blocklists)\n",
    "        payload = json.dumps(request_body)\n",
    "\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        print(response.status_code)\n",
    "        print(response.headers)\n",
    "        print(response.text)\n",
    "\n",
    "        res_content = response.json()\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise DetectionError(\n",
    "                res_content[\"error\"][\"code\"], res_content[\"error\"][\"message\"]\n",
    "            )\n",
    "\n",
    "        return res_content\n",
    "\n",
    "    def get_detect_result_by_category(\n",
    "        self, category: Category, detect_result: dict\n",
    "    ) -> Union[int, None]:\n",
    "        \"\"\"\n",
    "        Gets the detection result for the given category from the Content Safety API response.\n",
    "\n",
    "        Args:\n",
    "        - category (Category): The category to get the detection result for.\n",
    "        - detect_result (dict): The Content Safety API response.\n",
    "\n",
    "        Returns:\n",
    "        - Union[int, None]: The detection result for the given category, or None if it is not found.\n",
    "        \"\"\"\n",
    "        category_res = detect_result.get(\"categoriesAnalysis\", None)\n",
    "        for res in category_res:\n",
    "            if category.name == res.get(\"category\", None):\n",
    "                return res\n",
    "        raise ValueError(f\"Invalid Category {category}\")\n",
    "\n",
    "    def make_decision(\n",
    "        self,\n",
    "        detection_result: dict,\n",
    "        reject_thresholds: dict[Category, int],\n",
    "    ) -> Decision:\n",
    "        \"\"\"\n",
    "        Makes a decision based on the Content Safety API response and the specified reject thresholds.\n",
    "        Users can customize their decision-making method.\n",
    "\n",
    "        Args:\n",
    "        - detection_result (dict): The Content Safety API response.\n",
    "        - reject_thresholds (dict[Category, int]): The reject thresholds for each category.\n",
    "\n",
    "        Returns:\n",
    "        - Decision: The decision based on the Content Safety API response and the specified reject thresholds.\n",
    "        \"\"\"\n",
    "        action_result = {}\n",
    "        final_action = Action.Accept\n",
    "        for category, threshold in reject_thresholds.items():\n",
    "            if threshold not in (-1, 0, 2, 4, 6):\n",
    "                raise ValueError(\"RejectThreshold can only be in (-1, 0, 2, 4, 6)\")\n",
    "\n",
    "            cate_detect_res = self.get_detect_result_by_category(\n",
    "                category, detection_result\n",
    "            )\n",
    "            if cate_detect_res is None or \"severity\" not in cate_detect_res:\n",
    "                raise ValueError(f\"Can not find detection result for {category}\")\n",
    "\n",
    "            severity = cate_detect_res[\"severity\"]\n",
    "            action = (\n",
    "                Action.Reject\n",
    "                if threshold != -1 and severity >= threshold\n",
    "                else Action.Accept\n",
    "            )\n",
    "            action_result[category] = action\n",
    "            if action.value > final_action.value:\n",
    "                final_action = action\n",
    "\n",
    "        if (\n",
    "            \"blocklistsMatch\" in detection_result\n",
    "            and detection_result[\"blocklistsMatch\"]\n",
    "            and len(detection_result[\"blocklistsMatch\"]) > 0\n",
    "        ):\n",
    "            final_action = Action.Reject\n",
    "\n",
    "        print(final_action.name)\n",
    "        print(action_result)\n",
    "\n",
    "        return Decision(final_action, action_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ad1489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588116\n",
      "200\n",
      "{'Transfer-Encoding': 'chunked', 'Content-Type': 'application/json; charset=utf-8', 'csp-billing-usage': 'CognitiveServices.ContentSafety.Image:Analyze=1', 'api-supported-versions': '2023-04-30-preview,2023-05-30-preview,2023-10-01,2023-10-15-preview,2023-10-30-preview,2024-02-15-preview,2024-03-30-preview,2024-09-01,2024-09-15-preview,2024-09-30-preview', 'x-envoy-upstream-service-time': '169', 'apim-request-id': '66c4af1b-2ad3-4005-bb86-165598938cd5', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ms-region': 'East US', 'Date': 'Wed, 14 May 2025 01:54:12 GMT'}\n",
      "{\"categoriesAnalysis\":[{\"category\":\"Hate\",\"severity\":0},{\"category\":\"SelfHarm\",\"severity\":0},{\"category\":\"Sexual\",\"severity\":6},{\"category\":\"Violence\",\"severity\":0}]}\n",
      "Reject\n",
      "{<Category.Hate: 1>: <Action.Accept: 1>, <Category.SelfHarm: 2>: <Action.Accept: 1>, <Category.Sexual: 3>: <Action.Reject: 2>, <Category.Violence: 4>: <Action.Accept: 1>}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Replace the placeholders with your own values\n",
    "    endpoint = os.environ.get(\"AZURE_CONTENT_SAFETY_ENDPOINT\")\n",
    "    subscription_key = os.environ.get(\"AZURE_CONTENT_SAFETY_API_KEY\")\n",
    "    api_version = \"2024-09-01\"\n",
    "\n",
    "\n",
    "    # Initialize the ContentSafety object\n",
    "    content_safety = ContentSafety(endpoint, subscription_key, api_version)\n",
    "\n",
    "    # Set the media type and blocklists\n",
    "    media_type = MediaType.Image\n",
    "    blocklists = []\n",
    "\n",
    "    # Set the content to be tested\n",
    "    # Load an image from file and convert to base64\n",
    "    image_path = Path(\"./data/content_filter/image1.jpeg\")  # Replace with your image path\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "        content = base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "    print(len(content))\n",
    "\n",
    "    # Detect content safety\n",
    "    detection_result = content_safety.detect(media_type, content, blocklists)\n",
    "\n",
    "    # Set the reject thresholds for each category\n",
    "    reject_thresholds = {\n",
    "        Category.Hate: 4,\n",
    "        Category.SelfHarm: 4,\n",
    "        Category.Sexual: 4,\n",
    "        Category.Violence: 4,\n",
    "    }\n",
    "\n",
    "    # Make a decision based on the detection result and reject thresholds\n",
    "    decision_result = content_safety.make_decision(detection_result, reject_thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1afdfcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"categoriesAnalysis\": [\n",
      "        {\n",
      "            \"category\": \"Hate\",\n",
      "            \"severity\": 0\n",
      "        },\n",
      "        {\n",
      "            \"category\": \"SelfHarm\",\n",
      "            \"severity\": 0\n",
      "        },\n",
      "        {\n",
      "            \"category\": \"Sexual\",\n",
      "            \"severity\": 6\n",
      "        },\n",
      "        {\n",
      "            \"category\": \"Violence\",\n",
      "            \"severity\": 0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print detection_result JSON with indentation\n",
    "print(json.dumps(detection_result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5973ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
