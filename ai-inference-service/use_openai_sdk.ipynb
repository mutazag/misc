{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import base64\n",
    "import json\n",
    "from openai import AzureOpenAI  \n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://magoai-eastus2.openai.azure.com/\")  \n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"o3-mini\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI Service client with Entra ID authentication\n",
    "token_provider = get_bearer_token_provider(  \n",
    "    DefaultAzureCredential(),  \n",
    "    \"https://cognitiveservices.azure.com/.default\"  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(  \n",
    "    azure_endpoint=endpoint,  \n",
    "    azure_ad_token_provider=token_provider,  \n",
    "    api_version=\"2024-12-01-preview\",  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message(role, content):  \n",
    "    return {\"role\": role, \"content\": \n",
    "            [{\"type\": \"text\", \n",
    "             \"text\": content}]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = [\n",
    "    {\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are an AI assistant that helps people find information.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"role\": \"developer\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"You are an AI assistant that helps people find information.\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"It takes 2 hours to dry one shirt, how many hours does it take to dry 8 shirts?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "messages = chat_prompt \n",
    "messages.append(message(role=\"user\", content=\"It takes 2 hours to dry one shirt, how many hours does it take to dry 8 shirts?\"))\n",
    "\n",
    "print(json.dumps(messages, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-B9kUxj0bAtTUrKNun4J0fNQNuhZ3o\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"It still takes 2 hours. Drying is a process that happens concurrently rather than sequentially – if you can hang all 8 shirts to dry at the same time under the same conditions, then they will all finish drying in 2 hours.\\n\\nHowever, if you were forced to dry the shirts one after the other (which is usually not the case), then it would take 16 hours. But typically, when drying clothes, you’re drying them all at once.\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"protected_material_code\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"protected_material_text\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1741663391,\n",
      "  \"model\": \"o3-mini-2025-01-31\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": \"fp_ded0d14823\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 882,\n",
      "    \"prompt_tokens\": 43,\n",
      "    \"total_tokens\": 925,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 768,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(  \n",
    "    model=deployment,  \n",
    "    messages=messages,\n",
    "    max_completion_tokens=100000,\n",
    "    stop=None,  \n",
    "    stream=False ,\n",
    "\n",
    "    reasoning_effort=\"high\"\n",
    ")  \n",
    "  \n",
    "print(completion.to_json())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: It still takes 2 hours. Drying is a process that happens concurrently rather than sequentially – if you can hang all 8 shirts to dry at the same time under the same conditions, then they will all finish drying in 2 hours.\n",
      "\n",
      "However, if you were forced to dry the shirts one after the other (which is usually not the case), then it would take 16 hours. But typically, when drying clothes, you’re drying them all at once.\n",
      "reasoning effort: 768\n"
     ]
    }
   ],
   "source": [
    "print( f\"response: {completion.choices[0].message.content}\") \n",
    "print( f\"reasoning effort: {completion.usage.completion_tokens_details.reasoning_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
